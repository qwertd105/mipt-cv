{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Gz2KcdA324yC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.profiler as profiler\n",
        "import math\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import time\n",
        "import copy\n",
        "import timm\n",
        "from PIL import Image\n",
        "import gc\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка фиксированных сидов для воспроизводимости\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "metadata": {
        "id": "m0BPZ3v75PYI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lszvsOr5Vs3",
        "outputId": "f078550d-06e9-4b6d-e3b4-c3bc038c4e7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "BASE_LR = 0.001\n",
        "WARMUP_EPOCHS = 2  # Количество эпох для warmup\n",
        "NUM_CLASSES = 10\n",
        "VAL_SPLIT = 0.1\n",
        "OVERFIT_BATCHES = 3  # Для sanity check"
      ],
      "metadata": {
        "id": "VmnNRkSW5YQo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Метки классов CIFAR10\n",
        "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Пути для сохранения данных\n",
        "BASE_PATH = Path(\"./cifar10\")\n",
        "LOGS_PATH = Path(\"./logs\")\n",
        "RESULTS_PATH = Path(\"./results\")"
      ],
      "metadata": {
        "id": "Xh18-AR45cmz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH.mkdir(exist_ok=True)\n",
        "LOGS_PATH.mkdir(exist_ok=True)\n",
        "RESULTS_PATH.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "GLyteh9h5pTC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сборка датасета CIFAR10 для CNN"
      ],
      "metadata": {
        "id": "ynER7F4WsnqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n"
      ],
      "metadata": {
        "id": "aW1pr79r5q0r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для разделения данных на train/val\n",
        "def split_dataset(dataset, val_split=0.1):\n",
        "    dataset_size = len(dataset)\n",
        "    val_size = int(val_split * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    indices = list(range(dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
        "\n",
        "    return train_indices, val_indices"
      ],
      "metadata": {
        "id": "H9jWlKh0VQlk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_cifar10_imagefolder(root_dir, val_frac=0.1):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "    # Загрузка CIFAR-10\n",
        "    dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
        "\n",
        "    # Разбиение на train и val\n",
        "    train_indices, val_indices = split_dataset(dataset, val_frac)\n",
        "\n",
        "    # Сохранение изображений\n",
        "    def save_subset(indices, split_name):\n",
        "        for idx in indices:\n",
        "            img, label = dataset[idx]\n",
        "            class_name = dataset.classes[label]\n",
        "            save_dir = os.path.join(root_dir, split_name, class_name)\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "            if not isinstance(img, Image.Image):\n",
        "                img = transforms.ToPILImage()(img)\n",
        "            save_path = os.path.join(save_dir, f\"{idx}.png\")\n",
        "            img.save(save_path)\n",
        "\n",
        "    save_subset(train_indices, \"train\")\n",
        "    save_subset(val_indices, \"val\")\n",
        "\n",
        "    print(f\"CIFAR-10 сохранен в {root_dir}/train и {root_dir}/val\")\n",
        "    return root_dir"
      ],
      "metadata": {
        "id": "wPdK094R5vx0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_root = download_cifar10_imagefolder(BASE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AMOgXtF5yga",
        "outputId": "f746dc8f-2351-41e6-b0a6-2711f9ef7ea4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 сохранен в cifar10/train и cifar10/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(root=os.path.join(BASE_PATH, \"train\"), transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(BASE_PATH, \"val\"), transform=train_transform)"
      ],
      "metadata": {
        "id": "PEScWNi7508p"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_dataset = Subset(train_dataset, list(range(BATCH_SIZE * OVERFIT_BATCHES)))\n",
        "overfit_loader = DataLoader(\n",
        "    overfit_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "wa-A5fC26G2c"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Загрузка тестового датасета\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=str(BASE_PATH),\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "# Создаем DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "KCsVNqzS6WV9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline CNN"
      ],
      "metadata": {
        "id": "s5sRqlkqs1JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение модели CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jFlzyOKR6Zpf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train pipeline"
      ],
      "metadata": {
        "id": "4RUyW_oRs4eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, scheduler=None, num_epochs=10, experiment_name=\"default\"):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    writer = SummaryWriter(f'{LOGS_PATH}/{experiment_name}_{timestamp}')\n",
        "    print(f'{LOGS_PATH}/{experiment_name}_{timestamp}')\n",
        "\n",
        "    with profiler.profile(\n",
        "        activities=[\n",
        "            profiler.ProfilerActivity.CPU,\n",
        "            profiler.ProfilerActivity.CUDA,\n",
        "        ],\n",
        "        schedule=profiler.schedule(wait=1, warmup=1, active=3),\n",
        "        on_trace_ready=profiler.tensorboard_trace_handler(str(LOGS_PATH)),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True\n",
        "    ) as prof:\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "          print(f'Эпоха {epoch}/{num_epochs - 1}')\n",
        "          print('-' * 10)\n",
        "\n",
        "          for phase in ['train', 'val']:\n",
        "              if phase == 'train':\n",
        "                  model.train()\n",
        "              else:\n",
        "                  model.eval()\n",
        "\n",
        "              running_loss = 0.0\n",
        "              running_corrects = 0\n",
        "\n",
        "              with tqdm(dataloaders[phase], unit=\"batch\") as tepoch:\n",
        "                  for inputs, labels in tepoch:\n",
        "                      tepoch.set_description(f\"Epoch {epoch} - {phase}\")\n",
        "\n",
        "                      inputs = inputs.to(device)\n",
        "                      labels = labels.to(device)\n",
        "\n",
        "                      optimizer.zero_grad()\n",
        "\n",
        "                      with torch.set_grad_enabled(phase == 'train'):\n",
        "                          outputs = model(inputs)\n",
        "                          _, preds = torch.max(outputs, 1)\n",
        "                          loss = criterion(outputs, labels)\n",
        "\n",
        "                          if phase == 'train':\n",
        "                              loss.backward()\n",
        "                              optimizer.step()\n",
        "\n",
        "                      running_loss += loss.item() * inputs.size(0)\n",
        "                      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                  if phase == 'train' and scheduler is not None:\n",
        "                      scheduler.step()\n",
        "\n",
        "              epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "              epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "              print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "              writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
        "              writer.add_scalar(f'Accuracy/{phase}', epoch_acc, epoch)\n",
        "              current_lr = optimizer.param_groups[0]['lr']\n",
        "              print(\"curent_lr: \", current_lr)\n",
        "              writer.add_scalar('learning_rate/epoch', current_lr, epoch)\n",
        "\n",
        "              prof.step()\n",
        "\n",
        "              if phase == 'val' and epoch_acc > best_acc:\n",
        "                  best_acc = epoch_acc\n",
        "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Обучение завершено за {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Лучшая точность на валидации: {best_acc:.4f}')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return {\"best_model\": model,\n",
        "            \"SummaryWriter\": writer,\n",
        "            \"best_validation_accuracy\": best_acc}"
      ],
      "metadata": {
        "id": "Dx2AXYXe6lhs"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation pipeline"
      ],
      "metadata": {
        "id": "s7_0dehotBMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_with_confusion_matrix(model, test_loader, writer):\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Вычисление confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_predictions)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    cm_df = pd.DataFrame(\n",
        "        cm,\n",
        "        index=CIFAR10_CLASSES,\n",
        "        columns=CIFAR10_CLASSES\n",
        "    )\n",
        "\n",
        "    cm_norm_df = pd.DataFrame(\n",
        "        cm_normalized,\n",
        "        index=CIFAR10_CLASSES,\n",
        "        columns=CIFAR10_CLASSES\n",
        "    )\n",
        "\n",
        "    # Визуализация\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.heatmap(cm_norm_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "    plt.title(\"Normalized Confusion Matrix\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    confusion_matrix_path = RESULTS_PATH / \"confusion_matrix.png\"\n",
        "    plt.savefig(confusion_matrix_path)\n",
        "\n",
        "    writer.add_figure(\"Confusion Matrix\", plt.gcf())\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    report = classification_report(\n",
        "        all_targets,\n",
        "        all_predictions,\n",
        "        target_names=CIFAR10_CLASSES,\n",
        "        digits=3\n",
        "    )\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    report_path = RESULTS_PATH / \"classification_report.txt\"\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Считаем метрики\n",
        "    accuracy = sum(1 for p, t in zip(all_predictions, all_targets) if p == t) / len(all_targets)\n",
        "    f1_macro = f1_score(all_targets, all_predictions, average='macro')\n",
        "    f1_micro = f1_score(all_targets, all_predictions, average='micro')\n",
        "    f1_weighted = f1_score(all_targets, all_predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(\"f1_macro\", f1_macro)\n",
        "    print(\"f1_micro\", f1_micro)\n",
        "    print(\"f1_mweighted\", f1_weighted)\n",
        "\n",
        "    return cm, accuracy"
      ],
      "metadata": {
        "id": "y6sMqmmv8Mqr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check pipeline"
      ],
      "metadata": {
        "id": "gBZ8fbRdtUaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sanity_check(model, criterion, dataloader, optimizer, num_epochs, experiment_name):\n",
        "    print(\"\\n--- Starting Sanity Check (Overfitting test) ---\")\n",
        "\n",
        "    result = train_model(model, dataloaders, criterion, optimizer,\n",
        "                          num_epochs=num_epochs, experiment_name=experiment_name)\n",
        "    acc = result[\"best_validation_accuracy\"]\n",
        "\n",
        "    if acc > 98.0:\n",
        "        print(f\"Sanity check passed! Model successfully overfit with {acc:.2f}% accuracy\")\n",
        "\n",
        "    print(\"--- Sanity Check Completed ---\\n\")"
      ],
      "metadata": {
        "id": "2_uACL2v8M8_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity check и обучение CNN"
      ],
      "metadata": {
        "id": "ZhWnZl9etdUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Шедулер с вормапом и cosineAnnealing\n",
        "def warmup_then_cosine_scheduler(\n",
        "    optimizer,\n",
        "    start_factor,\n",
        "    warmup_duration,\n",
        "    total_steps\n",
        "):\n",
        "  warmup = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    end_factor=1.0,\n",
        "    total_iters=warmup_duration,\n",
        "  )\n",
        "\n",
        "  cos_annealing = optim.lr_scheduler.CosineAnnealingLR(\n",
        "      optimizer,\n",
        "      T_max=total_steps - warmup_duration,\n",
        "  )\n",
        "\n",
        "  scheduler = optim.lr_scheduler.SequentialLR(\n",
        "      optimizer,\n",
        "      schedulers=[warmup, cos_annealing],\n",
        "      milestones=[warmup_duration],\n",
        "  )\n",
        "  return scheduler"
      ],
      "metadata": {
        "id": "ZzMtZEqN-yCC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for sanity check\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=BASE_LR)\n",
        "\n",
        "start_factor = 0.1\n",
        "warmup_duration = len(train_loader) * 2\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "\n",
        "scheduler = warmup_then_cosine_scheduler(optimizer=optimizer, start_factor=start_factor, warmup_duration=warmup_duration, total_steps=total_steps)"
      ],
      "metadata": {
        "id": "z6naX40D-J28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\"train\": DataLoader(\n",
        "                            overfit_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle=True,\n",
        "                            num_workers=2,\n",
        "                            pin_memory=True\n",
        "                            ),\n",
        "                   \"val\": DataLoader(\n",
        "                          overfit_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )}\n",
        "\n",
        "sanity_check(model, criterion, dataloaders, optimizer, 50, \"cnn_sanity_check\")"
      ],
      "metadata": {
        "id": "QDmSTc43-nU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for train\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=BASE_LR)\n",
        "\n",
        "start_factor = 0.1\n",
        "warmup_duration = len(train_loader) * 2\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "\n",
        "scheduler = warmup_then_cosine_scheduler(optimizer=optimizer, start_factor=start_factor, warmup_duration=warmup_duration, total_steps=total_steps)\n",
        "dataloaders = {\"train\": train_loader, \"val\": val_loader}"
      ],
      "metadata": {
        "id": "dx03E7ReCuhk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = train_model(model, dataloaders, criterion, optimizer, scheduler=scheduler,\n",
        "                          num_epochs=NUM_EPOCHS, experiment_name=\"cnn_main\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkmsuM3_EMmD",
        "outputId": "abaff1f5-948e-4778-ff20-8b2c81bfd696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logs/cnn_main_20251002_201159\n",
            "Эпоха 0/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - train: 100%|██████████| 1407/1407 [00:41<00:00, 33.58batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.5011 Acc: 0.4591\n",
            "curent_lr:  0.00010031982942430704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - val: 100%|██████████| 157/157 [00:03<00:00, 46.16batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 1.2483 Acc: 0.5554\n",
            "curent_lr:  0.00010031982942430704\n",
            "Эпоха 1/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - train: 100%|██████████| 1407/1407 [01:17<00:00, 18.13batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.1835 Acc: 0.5788\n",
            "curent_lr:  0.00010063965884861408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - val: 100%|██████████| 157/157 [00:09<00:00, 17.33batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 1.0569 Acc: 0.6232\n",
            "curent_lr:  0.00010063965884861408\n",
            "Эпоха 2/9\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - train: 100%|██████████| 1407/1407 [01:27<00:00, 16.07batch/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm, test_accuracy = evaluate_model_with_confusion_matrix(model, test_loader, result[\"SummaryWriter\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jt693dPFAKz",
        "outputId": "23a8e259-6ceb-45fd-df29-30ce416ad2f0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane      0.762     0.777     0.769      1000\n",
            "  automobile      0.823     0.858     0.840      1000\n",
            "        bird      0.731     0.528     0.613      1000\n",
            "         cat      0.634     0.433     0.515      1000\n",
            "        deer      0.671     0.703     0.687      1000\n",
            "         dog      0.552     0.764     0.641      1000\n",
            "        frog      0.770     0.856     0.811      1000\n",
            "       horse      0.788     0.795     0.791      1000\n",
            "        ship      0.885     0.788     0.834      1000\n",
            "       truck      0.765     0.835     0.799      1000\n",
            "\n",
            "    accuracy                          0.734     10000\n",
            "   macro avg      0.738     0.734     0.730     10000\n",
            "weighted avg      0.738     0.734     0.730     10000\n",
            "\n",
            "\n",
            "Test Accuracy: 73.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка датасета для Vit"
      ],
      "metadata": {
        "id": "kYkevTh-uFEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_transform_vit = transforms.Compose([\n",
        "    transforms.Resize(224),  # Масштабируем до размера, который ожидает ViT\n",
        "    transforms.RandomCrop(224, padding=16),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform_vit = transforms.Compose([\n",
        "    transforms.Resize(224),  # Масштабируем до размера, который ожидает ViT\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "8X5dBhB-JRX_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_vit = datasets.ImageFolder(root=os.path.join(BASE_PATH, \"train\"), transform=train_transform_vit)\n",
        "val_dataset_vit = datasets.ImageFolder(root=os.path.join(BASE_PATH, \"val\"), transform=train_transform_vit)"
      ],
      "metadata": {
        "id": "YVsE-K2iKajH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overfit_dataset_vit = Subset(train_dataset_vit, list(range(BATCH_SIZE * OVERFIT_BATCHES)))\n",
        "overfit_loader_vit = DataLoader(\n",
        "    overfit_dataset_vit,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "O2g5sCZ1KeDE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Создаем DataLoaders\n",
        "train_loader_vit = DataLoader(\n",
        "    train_dataset_vit,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader_vit = DataLoader(\n",
        "    val_dataset_vit,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "uhLZpWMFKmhE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT Init, sanity check and train"
      ],
      "metadata": {
        "id": "zq6WfyfEuJm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vit = timm.create_model('vit_tiny_patch16_224', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBc8R-TPMPHv",
        "outputId": "2085c2bc-20a9-4315-a267-ce649b072bb8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sanity check"
      ],
      "metadata": {
        "id": "YffSgXSmudIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for param in model_vit.parameters():\n",
        "    param.requires_grad = False\n",
        "embedding_size = model_vit.head.in_features\n",
        "\n",
        "model_vit.head = nn.Linear(embedding_size, NUM_CLASSES)\n",
        "\n",
        "for param in model_vit.head.parameters():\n",
        "    param.requires_grad = True\n",
        "model_vit = model_vit.to(device)"
      ],
      "metadata": {
        "id": "Pxm4iwxuMUk_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_vit.parameters(), lr=0.01)\n",
        "\n",
        "start_factor = 0.1\n",
        "warmup_duration = 2\n",
        "total_steps = NUM_EPOCHS\n",
        "\n",
        "scheduler = warmup_then_cosine_scheduler(optimizer=optimizer, start_factor=start_factor, warmup_duration=warmup_duration, total_steps=total_steps)"
      ],
      "metadata": {
        "id": "MXfsQ2KvL9FA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\"train\": DataLoader(\n",
        "                            overfit_dataset_vit,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle=True,\n",
        "                            num_workers=2,\n",
        "                            pin_memory=True\n",
        "                            ),\n",
        "                   \"val\": DataLoader(\n",
        "                          overfit_dataset_vit,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )}\n",
        "\n",
        "sanity_check(model_vit, criterion, dataloaders, optimizer, 50, \"vit_sanity_check\")"
      ],
      "metadata": {
        "id": "RAy5RppkLBFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train"
      ],
      "metadata": {
        "id": "6V7QqvfmufJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vit = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
        "for param in model_vit.parameters():\n",
        "    param.requires_grad = False\n",
        "embedding_size = model_vit.head.in_features\n",
        "\n",
        "model_vit.head = nn.Linear(embedding_size, NUM_CLASSES)\n",
        "\n",
        "for param in model_vit.head.parameters():\n",
        "    param.requires_grad = True\n",
        "model_vit = model_vit.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "params_to_update = [p for p in model_vit.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
        "\n",
        "start_factor = 0.1\n",
        "warmup_duration = 1\n",
        "total_steps = 1\n",
        "\n",
        "scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=start_factor,\n",
        "    end_factor=1.0,\n",
        "    total_iters=warmup_duration,\n",
        "  )"
      ],
      "metadata": {
        "id": "NpElOlmFLYt7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders_vit = {\"train\": train_loader_vit, \"val\": val_loader_vit}"
      ],
      "metadata": {
        "id": "X7drHLqYOTNo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wHYKDW25cAJK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_vit = train_model(model_vit, dataloaders_vit, criterion, optimizer, scheduler=scheduler,\n",
        "                          num_epochs=1, experiment_name=\"vit_main\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flm5JYRqNkZ8",
        "outputId": "df361584-f0dc-45c6-a7b3-2bec7092593b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logs/vit_main_20251002_195047\n",
            "Эпоха 0/0\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - train: 100%|██████████| 1407/1407 [01:58<00:00, 11.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.6482 Acc: 0.7818\n",
            "curent_lr:  0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0 - val: 100%|██████████| 157/157 [00:14<00:00, 10.56batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.5348 Acc: 0.8120\n",
            "curent_lr:  0.01\n",
            "Обучение завершено за 2m 14s\n",
            "Лучшая точность на валидации: 0.8120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_vit = datasets.CIFAR10(\n",
        "    root=str(BASE_PATH),\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform_vit\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZuOlXMGNvhy",
        "outputId": "df90a9f5-652f-4e44-9879-2dadd055ec88"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:26<00:00, 6.39MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader_vit = DataLoader(\n",
        "    test_dataset_vit,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "El95kFfRffov"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### eval"
      ],
      "metadata": {
        "id": "At0JiBp2uh9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm, test_accuracy = evaluate_model_with_confusion_matrix(model_vit, test_loader_vit, result_vit[\"SummaryWriter\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O66ud98KfQN0",
        "outputId": "e89724a8-68c2-4fc2-b44e-17eff98824ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane      0.823     0.818     0.820      1000\n",
            "  automobile      0.890     0.867     0.878      1000\n",
            "        bird      0.845     0.763     0.802      1000\n",
            "         cat      0.803     0.565     0.663      1000\n",
            "        deer      0.820     0.789     0.804      1000\n",
            "         dog      0.651     0.860     0.741      1000\n",
            "        frog      0.786     0.929     0.852      1000\n",
            "       horse      0.883     0.814     0.847      1000\n",
            "        ship      0.866     0.883     0.874      1000\n",
            "       truck      0.850     0.864     0.857      1000\n",
            "\n",
            "    accuracy                          0.815     10000\n",
            "   macro avg      0.822     0.815     0.814     10000\n",
            "weighted avg      0.822     0.815     0.814     10000\n",
            "\n",
            "\n",
            "Test Accuracy: 81.52%\n",
            "f1_macro 0.8138462249930718\n",
            "f1_micro 0.8152\n",
            "f1_mweighted 0.8138462249930717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ieXjlTk0sWh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}